# Create the to_day function
to_day <- function(x) {
 x*24
}

# Create a list containing both vectors: all_visits
all_visits <- list(visit_a, visit_b)

# Convert to daily number of visits: all_visits_day
all_visits_day <- map(all_visits, to_day)

# Map the mean() function and output a numeric vector 
map_dbl(all_visits_day, mean)

############### sum of elements list by list (3 lists)

# Create all_tests list  and modify with to_day() function
all_tests <- list(visit_a, visit_b, visit_c)
all_tests_day <- map(all_tests, to_day)

# Plot all_tests_day with map
map(all_tests_day, barplot)

# Plot all_tests_day
walk(all_tests_day, barplot)

# Get the sum, of the all_tests_day list, element by element, and check its class
sum_all <- pmap(all_tests_day, sum)
class(sum_all)

map_dbl(all_tests_day, sum)
pmap_dbl(all_tests_day, sum)



###############Creating mappers
# Turn visit_a into daily number using an anonymous function
map(visit_a, function(x) {
  x*24
})

# Turn visit_a into daily number of visits by using a mapper
map(visit_a, ~.x*24)

# Create a mapper object called to_day
to_day <- as_mapper(~.x*24)

# Use it on the three vectors
map(visit_a, to_day)
map(visit_b, to_day)
map(visit_c, to_day)



# Round visit_a to the nearest tenth with a mapper
map_dbl(visit_a, ~ round(.x, -1))

# Create to_ten, a mapper that rounds to the nearest tenth
to_ten <- as_mapper(~ round(.x, -1))

# Map to_ten on visit_b
map_dbl(visit_b, to_ten)

# Map to_ten on visit_c
map_dbl(visit_c, to_ten)


############## Keep and discard
# Create a mapper that test if .x is more than 100 
is_more_than_hundred <- as_mapper(~ .x > 100)

# Use this mapper with keep() on the all_visits object 
map(all_visits, ~ keep(.x, is_more_than_hundred))

# Use the  day vector to set names to all_list
day <- c("mon", "tue", "wed", "thu", "fri", "sat", "sun")
full_visits_named <- map(all_visits, ~ set_names(.x, day))

# Use this mapper with keep() 
map(full_visits_named, ~ keep(.x, is_more_than_hundred))

# Set the name of each subvector
day <- c("mon", "tue", "wed", "thu", "fri", "sat", "sun")
all_visits_named <- map(all_visits, ~ set_names(.x, day))

# Create a mapper that will test if .x is over 100 
threshold <- as_mapper(~.x > 100)

# Run this mapper on the all_visits_named object: group_over
group_over <- map(all_visits_named, ~ keep(.x, threshold))

# Run this mapper on the all_visits_named object: group_under
group_under <-  map(all_visits_named, ~ discard(.x, threshold))


################# Predicate

# Create a threshold variable, set it to 160
threshold <- 160

# Create a mapper that tests if .x is over the defined threshold
over_threshold <- as_mapper(~ .x > threshold)

# Are all elements in every all_visits vectors over the defined threshold? 
map(all_visits, ~ every(.x, over_threshold))

# Are some elements in every all_visits vectors over the defined threshold? 
map(all_visits, ~ some(.x, over_threshold))



#############################Example
# Create a safe version of read_lines()
safe_read <- safely(read_lines)

# Map it on the urls vector
res <- map(urls, safe_read)

# Set the name of the results to `urls`
named_res <- set_names(res, urls)

# Extract only the "error" part of each sublist
map(named_res, "error")

###############################Example 2
# Create a safe version of read_lines()
safe_read <- safely(read_lines)

# Code a function that discard() the NULL from safe_read()
safe_read_discard <- function(url){
  safe_read(url) %>%
    discard(is.null)
}

# Map this function on the url list
res <- map(urls, safe_read_discard)+



#################Possible
# Create a possibly() version of read_lines()
possible_read <- possibly(read_lines, otherwise = 404)

# Map this function on urls, pipe it into set_names()
res <- map(urls, possible_read) %>% set_names(urls)

# Paste each element of the list 
res_pasted <- map(res, paste, collapse = " ")

# Keep only the elements which are equal to 404
keep(res_pasted, ~ .x == 404)

######one function call of possibly

url_tester <- function(url_list){
  url_list %>%
    # Map a version of read_lines() that otherwise returns 404
    map( possibly(read_lines, otherwise = 404) ) %>%
    # Set the names of the result
    set_names( urls ) %>% 
    # paste() and collapse each element
    map(paste, collapse = " ") %>%
    # Remove the 404 
    discard(~ .x == 404) %>%
    names() # Will return the names of the good ones
}

# Try this function on the urls object
url_tester(urls)


###Another function call 
# Complete the function definition
url_tester <- function(url_list, type = c("result", "error")) {
  type <- match.arg(type)
  url_list %>%
    # Apply safe_read to each URL
    map(safe_read) %>%
    # Set the names to the URLs
    set_names(url_list) %>%
    # Transpose into a list of $result and $error
    transpose()  %>%
    # Pluck the type element
    pluck(type) 
}


### another example using compact() to remove NULL
url_tester <- function(url_list){
  url_list %>%
    # Map a version of GET() that would otherwise return NULL 
    map(possibly(GET, otherwise = NULL) ) %>%
    # Set the names of the result
    set_names( urls ) %>%
    # Remove the NULL
    compact() %>%
    # Extract all the "status_code" elements
    pluck("status_code")
}

# Try this function on the urls object
url_tester(urls)





################# Better code with purr (CH4)


#### Compose function
# Launch purrr and httr
library(purrr)
library(httr)

# Compose a status extractor 
status_extract <- compose(status_code, GET)

# Try with "https://thinkr.fr" & "http://datacamp.com"
status_extract("https://thinkr.fr")
status_extract("http://datacamp.com")

# Map it on the urls vector, return a vector of numbers
map_dbl(urls, status_extract)


#### Negate and compose
# Negate the %in% function 
`%not_in%` <- negate(`%in%`)

# Compose a status extractor 
extract_status <- compose(status_code, GET)

# Complete the function definition
strict_code <- function(url) {
  # Extract the status of the URL
  code <- extract_status(url)
  # If code is not in the acceptable range ...
  if(code %not_in% 200:203) {
    # then return NA
    return(NA)
  }
  code
}


## ## NEgate function
# Map the strict_code function on the urls vector
res <- map(urls, strict_code)

# Set the names of the results using the urls vector
res_named <- set_names(res, urls)

# Negate the is.na function
is_not_na <- negate(is.na)

# Run is_not_na on the results
is_not_na(res_named)


#### Partial and Compose 

# Create a partial version of html_nodes(), with the css param set to "a"
get_a <- partial(html_nodes, css = "a")

# Create href(), a partial version of html_attr()
href <- partial(html_attr, name = "href")

# Combine href(), get_a(), and read_html()
get_links <- compose(href, get_a, read_html)

# Map get_links() to the urls list
res <- map(urls, get_links) %>%
  set_names(urls)

# See the result
res



#### Combining map with dplyr [[ Creating tiblle with list columns]]
# Create a "links" columns, by mapping get_links() on urls
df2 <- df %>%
  mutate(links = map(urls, get_links)) 

# Print df2 to see what it looks like
print(df2)

# unnest() df2 to have a tidy dataframe
df2 %>%
  unnest()



###############
###############
Final Chapter 
###############
###############


### 1
# Print the first element of the list to the console 
rstudioconf[[1]]

# Create a sublist of non-retweets
non_rt <- discard(rstudioconf, "is_retweet")

# Extract the favorite count element of each non_rt sublist
fav_count <- map_int(non_rt, "favorite_count")

# Get the median of favorite_count for non_rt
map_dbl(fav_count, median)



#### 2
# Keep the RT, extract the user_id, remove the duplicate
rt <- keep(rstudioconf, "is_retweet") %>%
  map("user_id") %>% 
  unique()


# Remove the RT, extract the user id, remove the duplicate
non_rt <- discard(rstudioconf, "is_retweet") %>%
  map("user_id") %>% 
  unique()

# Determine the total number of users
union(rt, non_rt) %>% length()

# Determine the number of users who has just retweeted
setdiff(rt, non_rt) %>% length()



###### 3
# Prefill mean() with na.rm, and round() with digits = 1
mean_na_rm <- partial(mean, na.rm = TRUE)
round_one <- partial(round, digits = 1)

# Compose a rounded_mean function
rounded_mean <- compose(round_one, mean_na_rm)

# Extract the non retweet  
non_rt <- discard(rstudioconf, "is_retweet")

# Extract "favorite_count", and pass it to rounded_mean()
non_rt %>%
  map_dbl("favorite_count") %>%
  rounded_mean()


##### 4
# Combine as_vector(), compact(), and flatten()
flatten_to_vector <- compose(as_vector, compact, flatten)

# Complete the fonction
extractor <- function(list, what = "mentions_screen_name"){
  map( list , what ) %>%
    flatten_to_vector()
}

# Create six_most, with tail(), sort(), and table()
six_most <- compose(tail, sort, table)

# Run extractor() on rstudioconf
extractor(rstudioconf) %>% 
  six_most()


###### 5
# Extract the "urls_url" elements, and flatten() the result
urls_clean <- map(rstudioconf, "urls_url") %>%
  flatten()

# Remove the NULL
compact_urls <- compact(urls_clean)

# Create a mapper that detects the patten "github"
has_github <- as_mapper(~ str_detect(.x, "github"))

# Look for the "github" pattern, and sum the result
has_github( compact_urls, has_github ) %>%
  sum()


###### 6
# From previous step
str_prop_detected <- function(vec, pattern) {
  vec %>%
    str_detect(pattern) %>%
    mean()
} 
flatten_and_compact <- compose(compact, flatten)

rstudioconf %>%
  # From each element, extract "urls_url"
  map("urls_url") %>%
  # Flatten and compact
  flatten_and_compact() %>% 
  # Get proportion of URLs containing "github"
  str_prop_detected("github")


##### 7
# Create mean_above, a mapper that tests if .x is over 3.3
mean_above <- as_mapper(~ .x > 3.3)

# Prefil map_at() with "retweet_count", mean_above for above, 
# and mean_above negation for below
above <- partial(map_at, .at = "retweet_count", .f := mean_above )
below <- partial(map_at, .at = "retweet_count", .f := negate(mean_above) )

# Map above() and below() on non_rt, keep the "retweet_count"
ab <- map(non_rt, above) %>% keep("retweet_count")
bl <- map(non_rt, below) %>% keep("retweet_count")

# Compare the size of both elements
length(ab)
length(bl)


##### 8 
# Get the max() of "retweet_count" 
max_rt <- map_dbl(non_rt, "retweet_count") %>% 
  max()

# Prefill map_at() with a mapper testing if .x equal max_rt
max_rt_calc <- partial(map_at, .at = "retweet_count", .f := ~ .x == max_rt )

res <- non_rt %>%
  # Call max_rt_calc() on each element
  map(max_rt_calc) %>% 
  # Keep elements where retweet_count is non-zero
  keep("retweet_count") %>% 
  # Flatten it
  flatten()

# Print the "screen_name" and "text" of the result
res$screen_name
res$text 